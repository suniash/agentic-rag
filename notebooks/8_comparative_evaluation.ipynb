{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Notebook 8 \u00b7 Comparative Evaluation\n",
        "\n",
        "With multiple pipelines in place, this notebook benchmarks them across quality and latency metrics using a small evaluation set."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "source": [
        "import pandas as pd\n",
        "from ragas import evaluate as ragas_evaluate\n",
        "from ragas.metrics import answer_relevancy, faithfulness\n",
        "\n",
        "from pprint import pprint\n",
        "\n",
        "from shared import (\n",
        "    DEFAULT_MODEL,\n",
        "    RetrievalContext,\n",
        "    build_baseline_chain,\n",
        "    build_retrieval_context,\n",
        "    pretty_print_json,\n",
        "    time_execution,\n",
        ")\n"
      ],
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "source": [
        "evaluation_set = pd.DataFrame([\n",
        "    {'question': 'How do I transfer workspace ownership?'},\n",
        "    {'question': 'What are the storage limits on the Enterprise tier?'},\n",
        "    {'question': 'Can contractors access billing dashboards?'},\n",
        "])\n",
        "context = build_retrieval_context(top_k=4)\n",
        "qa_chain = build_baseline_chain(context.retriever)\n"
      ],
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "source": [
        "def run_baseline(row):\n",
        "    return qa_chain.run(row.question)\n",
        "\n",
        "evaluation_set['baseline_answer'] = evaluation_set.apply(run_baseline, axis=1)\n",
        "print(evaluation_set.head())\n"
      ],
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "source": [
        "retrieved_corpora = [\n",
        "    '\\n\\n'.join(doc.page_content for doc in context.retriever.get_relevant_documents(question))\n",
        "    for question in evaluation_set['question']\n",
        "]\n",
        "metric_suite = [faithfulness, answer_relevancy]\n",
        "ragas_report = ragas_evaluate(\n",
        "    questions=evaluation_set['question'].tolist(),\n",
        "    answers=evaluation_set['baseline_answer'].tolist(),\n",
        "    contexts=retrieved_corpora,\n",
        "    metrics=metric_suite,\n",
        ")\n",
        "ragas_report\n"
      ],
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Visual analysis\n",
        "\n",
        "Use the resulting `ragas_report` to chart how each pipeline performs. Extend this notebook by importing the functions defined in previous notebooks and repeating the evaluation for planner/executor, reflective, and verified variants."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}