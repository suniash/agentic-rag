{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Notebook 2 \u00b7 Self-Improving Micro-Loop\n",
        "\n",
        "This notebook demonstrates the smallest possible agentic feedback loop.\n",
        "\n",
        "You will:\n",
        "\n",
        "1. Run a retrieval \u2192 answer pipeline over a tiny local corpus.\n",
        "2. Evaluate the answer with a lightweight keyword proxy.\n",
        "3. Let an improver agent write a memory note capturing the missing facts.\n",
        "4. Re-run retrieval so the new memory boosts the second attempt.\n",
        "\n",
        "> \ud83e\uddea **Tip:** This entire demo runs offline. You can safely extend it with real embeddings, LLM calls, and groundedness metrics later."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Setup\n",
        "\n",
        "The corpus lives in `../data/mini_corpus.jsonl`. Memory notes are persisted to\n",
        "`../data/memory_notes.jsonl` so you can inspect how knowledge accumulates between\n",
        "iterations. Delete the memory file if you want to reset the loop."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "from __future__ import annotations\n",
        "\n",
        "import json\n",
        "import uuid\n",
        "from collections import Counter\n",
        "from dataclasses import dataclass\n",
        "from pathlib import Path\n",
        "from typing import Iterable, List, Tuple\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "DATA_PATH = (Path(\"../data/mini_corpus.jsonl\").resolve())\n",
        "MEM_PATH = (Path(\"../data/memory_notes.jsonl\").resolve())\n",
        "EMBED_DIM = 256\n",
        "TOP_K = 3\n",
        "SCORE_THRESHOLD = 0.75\n",
        "\n",
        "DATA_PATH.parent.mkdir(parents=True, exist_ok=True)\n",
        "if not MEM_PATH.exists():\n",
        "    MEM_PATH.touch()"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "def load_jsonl(path: Path) -> List[dict]:\n",
        "    if not path.exists() or path.stat().st_size == 0:\n",
        "        return []\n",
        "    with path.open() as handle:\n",
        "        return [json.loads(line) for line in handle]\n",
        "\n",
        "\n",
        "corpus = load_jsonl(DATA_PATH)\n",
        "memory = load_jsonl(MEM_PATH)\n",
        "print(f\"Loaded {len(corpus)} base docs and {len(memory)} memory notes.\")"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "def toy_embed(text: str, dim: int = EMBED_DIM) -> np.ndarray:\n",
        "    tokens = text.lower().split()\n",
        "    counts = Counter(tokens)\n",
        "    vec = np.zeros(dim, dtype=np.float32)\n",
        "    for token, count in counts.items():\n",
        "        bucket = int(uuid.uuid5(uuid.NAMESPACE_DNS, token).hex, 16) % dim\n",
        "        vec[bucket] += float(count)\n",
        "    norm = np.linalg.norm(vec) + 1e-9\n",
        "    return vec / norm\n",
        "\n",
        "\n",
        "@dataclass\n",
        "class RetrievedDoc:\n",
        "    doc: dict\n",
        "    score: float\n",
        "\n",
        "\n",
        "def embed_document(doc: dict) -> np.ndarray:\n",
        "    return toy_embed(f\"{doc['title']} {doc['text']}\")\n",
        "\n",
        "\n",
        "ALL_DOCS: List[dict] = corpus + memory\n",
        "DOC_EMBEDDINGS: List[np.ndarray] = [embed_document(doc) for doc in ALL_DOCS]\n",
        "\n",
        "\n",
        "def refresh_index() -> None:\n",
        "    global ALL_DOCS, DOC_EMBEDDINGS\n",
        "    ALL_DOCS = corpus + memory\n",
        "    DOC_EMBEDDINGS = [embed_document(doc) for doc in ALL_DOCS]\n",
        "\n",
        "\n",
        "refresh_index()"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "def retrieve(query: str, top_k: int = TOP_K) -> List[RetrievedDoc]:\n",
        "    query_vec = toy_embed(query)\n",
        "    sims = np.dot(DOC_EMBEDDINGS, query_vec)\n",
        "    top_indices = np.argsort(-sims)[:top_k]\n",
        "    return [RetrievedDoc(ALL_DOCS[i], float(sims[i])) for i in top_indices]\n",
        "\n",
        "\n",
        "def assemble_context(retrieved: Iterable[RetrievedDoc]) -> str:\n",
        "    chunks = []\n",
        "    for item in retrieved:\n",
        "        chunks.append(\n",
        "            f\"[{item.doc['id']}] {item.doc['title']} (score={item.score:.3f})\\n{item.doc['text']}\"\n",
        "        )\n",
        "    return \"\\n\\n\".join(chunks)\n",
        "\n",
        "\n",
        "def generate_answer(question: str, context: str) -> str:\n",
        "    return (\n",
        "        \"(SIMULATED ANSWER)\\n\"\n",
        "        + f\"Question: {question}\\n\"\n",
        "        + \"Context used:\n\"\n",
        "        + context\n",
        "        + \"\\n---\\nThe assistant summarises the retrieved notes above.\"\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "def eval_keywords(answer: str, required_phrases: Iterable[str]) -> Tuple[float, List[str]]:\n",
        "    answer_lower = answer.lower()\n",
        "    missing = [phrase for phrase in required_phrases if phrase.lower() not in answer_lower]\n",
        "    score = (len(required_phrases) - len(missing)) / max(1, len(required_phrases))\n",
        "    return score, missing\n",
        "\n",
        "\n",
        "def write_memory_note(question: str, missing_phrases: Iterable[str]) -> dict:\n",
        "    memory_note = {\n",
        "        \"id\": f\"mem-{uuid.uuid4().hex[:8]}\",\n",
        "        \"title\": f\"Memory Note: {question[:48]}\",\n",
        "        \"text\": \"Key additions: \" + \"; \".join(missing_phrases) + \". Reminder: revisit these concepts next time.\",\n",
        "    }\n",
        "    memory.append(memory_note)\n",
        "    with MEM_PATH.open(\"a\") as handle:\n",
        "        handle.write(json.dumps(memory_note) + \"\\n\")\n",
        "    refresh_index()\n",
        "    return memory_note"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "evaluation_plan = [\n",
        "    {\n",
        "        \"question\": \"Explain RAG and why retrieval reduces hallucination.\",\n",
        "        \"required\": [\"retrieval\", \"grounding\", \"context\", \"hallucination\"],\n",
        "    },\n",
        "    {\n",
        "        \"question\": \"What is the role of embeddings in a vector database?\",\n",
        "        \"required\": [\"embeddings\", \"similarity search\", \"vector database\"],\n",
        "    },\n",
        "]\n",
        "\n",
        "\n",
        "def rag_attempt(question: str):\n",
        "    retrieved = retrieve(question)\n",
        "    context = assemble_context(retrieved)\n",
        "    answer = generate_answer(question, context)\n",
        "    return answer, retrieved\n",
        "\n",
        "\n",
        "def run_feedback_loop(question: str, required_phrases: Iterable[str]):\n",
        "    answer1, retrieved1 = rag_attempt(question)\n",
        "    score1, missing = eval_keywords(answer1, required_phrases)\n",
        "    memory_note = None\n",
        "    if score1 < SCORE_THRESHOLD and missing:\n",
        "        memory_note = write_memory_note(question, missing)\n",
        "    answer2, retrieved2 = rag_attempt(question)\n",
        "    score2, _ = eval_keywords(answer2, required_phrases)\n",
        "    return {\n",
        "        \"question\": question,\n",
        "        \"attempt_1_score\": score1,\n",
        "        \"attempt_1_docs\": [doc.doc[\"id\"] for doc in retrieved1],\n",
        "        \"memory_written\": memory_note[\"id\"] if memory_note else None,\n",
        "        \"attempt_2_score\": score2,\n",
        "        \"attempt_2_docs\": [doc.doc[\"id\"] for doc in retrieved2],\n",
        "    }"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "reports = [run_feedback_loop(item[\"question\"], item[\"required\"]) for item in evaluation_plan]\n",
        "\n",
        "df = pd.DataFrame(reports)\n",
        "df"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "for item in reports:\n",
        "    print(f\"Question: {item['question']}\")\n",
        "    print(f\"  Attempt 1 score: {item['attempt_1_score']:.2f} | docs: {item['attempt_1_docs']}\")\n",
        "    print(f\"  Memory note: {item['memory_written']}\")\n",
        "    print(f\"  Attempt 2 score: {item['attempt_2_score']:.2f} | docs: {item['attempt_2_docs']}\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Next steps\n",
        "\n",
        "* Replace `toy_embed` with real embeddings such as `text-embedding-3-small` or\n",
        "  `sentence-transformers` to mirror production behaviour.\n",
        "* Swap the simulated summariser with a live LLM call via LangChain or the\n",
        "  OpenAI client.\n",
        "* Upgrade the keyword score to a groundedness metric like RAGAS.\n",
        "* Track additional telemetry (token cost, run metadata) so you can compare the\n",
        "  micro-loop against the larger agentic notebooks."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}