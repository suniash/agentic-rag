{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5558afa0",
   "metadata": {},
   "source": [
    "\n",
    "# Traditional RAG from Scratch\n",
    "\n",
    "Welcome to the warm-up notebook for this repository. The goal is to demystify the classic retrieval-augmented generation pipeline before we add agentic behaviours on top of it. Every step is fully transparent so you can connect the dots between data, maths, and code.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce61f242",
   "metadata": {},
   "source": [
    "\n",
    "## 1. Load a tiny knowledge base\n",
    "\n",
    "We will start with a short list of study notes about agentic RAG.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e47c344",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "documents = [\n",
    "    {\n",
    "        \"title\": \"Agent loops\",\n",
    "        \"text\": \"Agentic RAG adds planning loops that let the model break a task into steps.\"\n",
    "    },\n",
    "    {\n",
    "        \"title\": \"Classic pipeline\",\n",
    "        \"text\": \"Traditional RAG retrieves relevant context chunks and feeds them to a language model.\"\n",
    "    },\n",
    "    {\n",
    "        \"title\": \"Evaluation\",\n",
    "        \"text\": \"Measuring retrieval quality helps you understand if the context was helpful.\"\n",
    "    },\n",
    "    {\n",
    "        \"title\": \"Tooling\",\n",
    "        \"text\": \"Agents can call tools such as search, code execution, or calculators when a single prompt is not enough.\"\n",
    "    }\n",
    "]\n",
    "len(documents)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08421346",
   "metadata": {},
   "source": [
    "\n",
    "## 2. Tokenise the documents\n",
    "\n",
    "We build a very small tokenizer that lowercases the text and splits on letters only. This keeps the maths easy to follow.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2726b644",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import re\n",
    "\n",
    "def tokenize(text: str):\n",
    "    tokens = re.findall(r\"[a-zA-Z']+\", text.lower())\n",
    "    return tokens\n",
    "\n",
    "sample_tokens = tokenize(documents[0][\"text\"])\n",
    "sample_tokens[:10]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8a0e4b1",
   "metadata": {},
   "source": [
    "\n",
    "## 3. Build a vocabulary and compute document statistics\n",
    "\n",
    "We collect every unique token and count how often it appears in each document. This is the data we will use for scoring later.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c53a5357",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "\n",
    "# Build the vocabulary\n",
    "vocab = sorted({token for doc in documents for token in tokenize(doc[\"text\"])})\n",
    "word_index = {word: idx for idx, word in enumerate(vocab)}\n",
    "\n",
    "# Compute document frequencies\n",
    "term_counts = []\n",
    "df_counts = Counter()\n",
    "for doc in documents:\n",
    "    counts = Counter(tokenize(doc[\"text\"]))\n",
    "    term_counts.append(counts)\n",
    "    for token in counts:\n",
    "        df_counts[token] += 1\n",
    "\n",
    "N = len(documents)\n",
    "idf = {\n",
    "    term: np.log((1 + N) / (1 + df_counts[term])) + 1\n",
    "    for term in vocab\n",
    "}\n",
    "len(vocab), list(idf.items())[:5]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b47e1c9e",
   "metadata": {},
   "source": [
    "\n",
    "## 4. Vectorise documents with TF-IDF\n",
    "\n",
    "Now we convert each document into a numerical vector. Term Frequency–Inverse Document Frequency (TF-IDF) gives higher weight to words that are unique to a document.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49308346",
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorise(counts):\n",
    "    vector = np.zeros(len(vocab), dtype=float)\n",
    "    total_tokens = sum(counts.values()) or 1\n",
    "    for token, count in counts.items():\n",
    "        if token not in word_index:\n",
    "            continue\n",
    "        idx = word_index[token]\n",
    "        tf = count / total_tokens\n",
    "        vector[idx] = tf * idf[token]\n",
    "    return vector\n",
    "\n",
    "doc_matrix = np.vstack([vectorise(counts) for counts in term_counts])\n",
    "doc_matrix.shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "169d56d5",
   "metadata": {},
   "source": [
    "\n",
    "## 5. Retrieve context for a question\n",
    "\n",
    "We turn a user question into a vector and rank the documents with cosine similarity.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d90633f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def retrieve(query: str, top_k: int = 2):\n",
    "    query_counts = Counter(tokenize(query))\n",
    "    query_vector = vectorise(query_counts)\n",
    "    # Normalise vectors to avoid magnitude differences\n",
    "    doc_norms = np.linalg.norm(doc_matrix, axis=1)\n",
    "    query_norm = np.linalg.norm(query_vector)\n",
    "    similarities = (doc_matrix @ query_vector) / (doc_norms * (query_norm or 1))\n",
    "    scored = [\n",
    "        (score, doc)\n",
    "        for score, doc in zip(similarities, documents)\n",
    "    ]\n",
    "    ranked = sorted(scored, key=lambda item: item[0], reverse=True)\n",
    "    return ranked[:top_k]\n",
    "\n",
    "results = retrieve(\"How do agents plan their work?\")\n",
    "results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e09abf0",
   "metadata": {},
   "source": [
    "\n",
    "## 6. Compose a final answer\n",
    "\n",
    "In a production system this step would call a language model. To keep things simple we stitch together a response using the retrieved snippets.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "471a4c59",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def simple_answer(question: str, retrieved):\n",
    "    context = \"\n",
    "\".join(f\"- {doc['text']}\" for _, doc in retrieved)\n",
    "    return (\n",
    "        f\"Question: {question}\n",
    "\n",
    "Context used:\n",
    "{context}\n",
    "\n",
    "\"\n",
    "        \"Takeaway: Agentic RAG builds on this baseline by deciding when to loop, \"\n",
    "        \"call tools, or request more context before answering.\"\n",
    "    )\n",
    "\n",
    "print(simple_answer(\"How do agents plan their work?\", results))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cfb70c6",
   "metadata": {},
   "source": [
    "\n",
    "## 7. What changes in agentic RAG?\n",
    "\n",
    "Classic RAG stops after a single retrieve-and-respond cycle. Agentic RAG layers on abilities such as:\n",
    "\n",
    "* **Planning** – break the task into subtasks and decide which tool should handle each one.\n",
    "* **Reflection** – review an initial answer, critique it, and iterate until it meets a quality bar.\n",
    "* **Tool use** – call external APIs, code execution sandboxes, or search services when the retrieved text is not enough.\n",
    "* **Evaluation loops** – measure retrieval quality and response usefulness, then adjust prompts or data sources automatically.\n",
    "\n",
    "As you work through the subsequent notebooks, look for these extra loops and decisions. They reuse the same retrieval building block you built here, so understanding this foundation makes the agentic upgrades much easier to follow.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faa5c0b0",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "You now have a minimal, transparent RAG pipeline. Use it as a mental model when studying the agentic notebooks in this repo.\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
