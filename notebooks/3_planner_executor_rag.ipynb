{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Notebook 3 \u00b7 Planner \u2192 Executor Agentic RAG\n",
        "\n",
        "This notebook explores how a lightweight planner can orchestrate multiple retrieval steps. We lean on LangChain's `PlanAndExecute` helper to build a planner that decomposes a question into sub-queries before executing them with our baseline retrieval components."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The pattern is useful for multi-hop or analytical queries where a single search pass is unlikely to surface every fact required for a grounded answer."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "source": [
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "from langchain.agents.agent_toolkits import create_retriever_tool\n",
        "from langchain.experimental.plan_and_execute import (\n",
        "    PlanAndExecute,\n",
        "    load_agent_executor,\n",
        "    load_chat_planner,\n",
        ")\n",
        "from langchain_openai import ChatOpenAI as LangChainChatOpenAI\n",
        "\n",
        "from pprint import pprint\n",
        "\n",
        "from shared import (\n",
        "    DEFAULT_MODEL,\n",
        "    RetrievalContext,\n",
        "    build_baseline_chain,\n",
        "    build_retrieval_context,\n",
        "    pretty_print_json,\n",
        "    time_execution,\n",
        ")\n"
      ],
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "source": [
        "context = build_retrieval_context(top_k=5)\n",
        "retriever_tool = create_retriever_tool(\n",
        "    context.retriever,\n",
        "    'help_center_search',\n",
        "    'Use when you need detailed information about NimbusWorkspaces product behaviour, limits, or billing.',\n",
        ")\n",
        "planner_llm = LangChainChatOpenAI(model=DEFAULT_MODEL, temperature=0.2)\n",
        "executor_llm = LangChainChatOpenAI(model=DEFAULT_MODEL, temperature=0.0)\n",
        "\n",
        "planner = load_chat_planner(planner_llm)\n",
        "executor = load_agent_executor(executor_llm, [retriever_tool], verbose=True)\n",
        "plan_and_execute = PlanAndExecute(planner=planner, executor=executor)\n"
      ],
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "source": [
        "complex_question = (\n",
        "    'Compare the upgrade paths between the Team and Enterprise plans and list any approval requirements.'\n",
        ")\n",
        "response = plan_and_execute({'input': complex_question})\n",
        "print(response['output'])\n"
      ],
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## What to look for\n",
        "\n",
        "* Inspect the console logs above to see the planner's decomposition steps.\n",
        "* Experiment with higher temperature for the planner to increase exploration.\n",
        "* Replace the executor with a different tool stack (e.g. web search) to observe how planning adapts."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}