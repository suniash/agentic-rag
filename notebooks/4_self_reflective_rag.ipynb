{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Notebook 4 \u00b7 Self-Reflective RAG\n",
        "\n",
        "In this notebook the agent critiques its own answer. After generating an initial response, we use a critique prompt to decide whether to re-query the knowledge base for better evidence. This mirrors evaluation loops from frameworks like RAGAS."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "source": [
        "from langchain.prompts import ChatPromptTemplate\n",
        "from langchain_openai import ChatOpenAI as LangChainChatOpenAI\n",
        "\n",
        "from pprint import pprint\n",
        "\n",
        "from shared import (\n",
        "    DEFAULT_MODEL,\n",
        "    RetrievalContext,\n",
        "    build_baseline_chain,\n",
        "    build_retrieval_context,\n",
        "    pretty_print_json,\n",
        "    time_execution,\n",
        ")\n"
      ],
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "source": [
        "context = build_retrieval_context(top_k=4)\n",
        "qa_chain = build_baseline_chain(context.retriever)\n",
        "critic = LangChainChatOpenAI(model=DEFAULT_MODEL, temperature=0.0)\n"
      ],
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "source": [
        "critique_prompt = ChatPromptTemplate.from_template(\n",
        "    'Answer:\\n{answer}\\n\\nEvidence:\\n{evidence}\\n\\nDoes the answer need another retrieval pass? Reply with YES or NO and explain.'\n",
        ")\n"
      ],
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "source": [
        "def ask_with_reflection(question: str, max_retries: int = 2) -> str:\n",
        "    evidence_docs = context.retriever.get_relevant_documents(question)\n",
        "    evidence_text = '\\n\\n'.join(doc.page_content for doc in evidence_docs)\n",
        "    answer = qa_chain.run(question)\n",
        "\n",
        "    for _ in range(max_retries):\n",
        "        critique = critic(critique_prompt.format_messages(answer=answer, evidence=evidence_text))\n",
        "        if 'YES' not in critique.content.upper():\n",
        "            return answer + '\\n\\nCritique: ' + critique.content\n",
        "\n",
        "        evidence_docs = context.retriever.get_relevant_documents(question)\n",
        "        evidence_text = '\\n\\n'.join(doc.page_content for doc in evidence_docs)\n",
        "        answer = qa_chain.run(question)\n",
        "\n",
        "    return answer + '\\n\\nCritique after retries: ' + critique.content\n"
      ],
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "source": [
        "question = 'What happens if my workspace exceeds the seat limit mid-cycle?'\n",
        "print(ask_with_reflection(question))\n"
      ],
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Extension ideas\n",
        "\n",
        "* Swap the critic for `gpt-4.1` (non-mini) to see if judgments improve.\n",
        "* Compute a RAGAS faithfulness score after each iteration to guide retries.\n",
        "* Cache retrieval results to avoid redundant FAISS lookups."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}